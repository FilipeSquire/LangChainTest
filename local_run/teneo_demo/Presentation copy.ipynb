{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7747a0f2",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4c1fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4886357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipesilverio/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.CompositeElement at 0x17cbd0250>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0550>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0760>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0280>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0700>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0880>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0850>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0ac0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0820>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0a90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0130>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd09a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0bb0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0d90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0d60>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0fa0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0f10>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0e80>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0df0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0b20>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17b4a9f40>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cf8b7f0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cf8b880>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17b4b07f0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cf8b8e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x17cbd0370>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the data \n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "output_path = ''\n",
    "file_path = '/Users/felipesilverio/Documents/GitHub/LangChainTest/test2.pdf'\n",
    "\n",
    "chunks = partition_pdf(\n",
    "    filename= file_path,\n",
    "    infer_table_structure=True, #extracting table\n",
    "    strategy = 'hi_res', #mandatory to infer table\n",
    "\n",
    "    extract_image_block_types=['Image'], #add 'Table' to list to extract image of tables\n",
    "    # image_output_dir_path = output_path, #if None, images and tables will be saved as base64\n",
    "\n",
    "    extract_image_block_to_payload=True, #if true, extract base64 for API usage\n",
    "\n",
    "    chunking_strategy='by_title', #or basic\n",
    "    max_characters=10000, #default is 500\n",
    "    combine_text_under_n_chars=2000, #default is 0\n",
    "    new_after_n_chars=6000, #default is 0\n",
    ")\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f9bd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, texts, images, grouped_texts = [], [], [], []\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Work on the inner elements list (or the chunk itself if not composite)\n",
    "    element_list = (\n",
    "        chunk.metadata.orig_elements\n",
    "        if \"CompositeElement\" in str(type(chunk))\n",
    "        else [chunk]\n",
    "    )\n",
    "\n",
    "    idx = 0\n",
    "    while idx < len(element_list):\n",
    "        el = element_list[idx]\n",
    "\n",
    "        if \"Table\" in str(type(el)):\n",
    "            tables.append(el)\n",
    "\n",
    "        elif \"Image\" in str(type(el)):\n",
    "            images.append(el)\n",
    "\n",
    "        # ── Title + next-element grouping ───────────────────────────────────\n",
    "        elif \"Title\" in str(type(el)) and idx + 1 < len(element_list):\n",
    "            nxt = element_list[idx + 1]\n",
    "            if nxt.category not in (\"Title\", \"Table\", \"Image\"):\n",
    "                grouped_texts.append(f\"{el.text}\\n{nxt.text}\")\n",
    "                idx += 2          # skip the element we just merged\n",
    "                continue\n",
    "            else:\n",
    "                texts.append(el)  # title without suitable follower\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        else:\n",
    "            texts.append(el)\n",
    "\n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb66b458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Table at 0x34b1c0460>,\n",
       " <unstructured.documents.elements.Table at 0x34b1c0be0>,\n",
       " <unstructured.documents.elements.Table at 0x30ac1c520>,\n",
       " <unstructured.documents.elements.Table at 0x30ac2c730>,\n",
       " <unstructured.documents.elements.Table at 0x30ac2e790>,\n",
       " <unstructured.documents.elements.Table at 0x30ac33100>,\n",
       " <unstructured.documents.elements.Table at 0x302335af0>,\n",
       " <unstructured.documents.elements.Table at 0x302348ac0>,\n",
       " <unstructured.documents.elements.Table at 0x30234d0d0>,\n",
       " <unstructured.documents.elements.Table at 0x302348850>,\n",
       " <unstructured.documents.elements.Table at 0x30234d6d0>,\n",
       " <unstructured.documents.elements.Table at 0x30234da90>,\n",
       " <unstructured.documents.elements.Table at 0x30232b0d0>,\n",
       " <unstructured.documents.elements.Table at 0x302353f40>,\n",
       " <unstructured.documents.elements.Table at 0x302357340>,\n",
       " <unstructured.documents.elements.Table at 0x302357670>,\n",
       " <unstructured.documents.elements.Table at 0x302357b80>,\n",
       " <unstructured.documents.elements.Table at 0x30235ba30>,\n",
       " <unstructured.documents.elements.Table at 0x30235bf10>,\n",
       " <unstructured.documents.elements.Table at 0x302360310>,\n",
       " <unstructured.documents.elements.Table at 0x302368ac0>,\n",
       " <unstructured.documents.elements.Table at 0x302368490>,\n",
       " <unstructured.documents.elements.Table at 0x30236c1c0>,\n",
       " <unstructured.documents.elements.Table at 0x3023772e0>,\n",
       " <unstructured.documents.elements.Table at 0x3023779d0>,\n",
       " <unstructured.documents.elements.Table at 0x30237fb20>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, texts, images= [], [], []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if \"CompositeElement\" in str(type(chunk)):\n",
    "        chunk_els = chunk.metadata.orig_elements\n",
    "        for el in chunk_els:\n",
    "            if \"Table\" in str(type(el)):\n",
    "                tables.append(el)\n",
    "            elif \"Image\" in str(type(el)):\n",
    "                images.append(el)\n",
    "            elif \"Title\" in str(type(el)) and i + 1 < len(chunks):\n",
    "                nxt = chunks[i + 1]\n",
    "                if nxt.category not in (\"Title\", \"Table\", \"Image\"):\n",
    "                    combined = f\"{el.text}\\n{nxt.text}\"\n",
    "                    grouped_texts.append(combined)\n",
    "                    i += 2\n",
    "                    continue  # skip to element after next\n",
    "            else:\n",
    "                texts.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37647a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "vectorstore = Chroma(collection_name='multi_modal_rag', embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "#Storage layor\n",
    "store = InMemoryStore()\n",
    "id_key = 'doc_id'\n",
    "\n",
    "#retriever\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca6ee6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [str(uuid.uuid4()) for _ in grouped_texts]\n",
    "\n",
    "text_vectors = [\n",
    "    Document(page_content=txt, metadata={id_key: uid, \"type\": \"text\"})\n",
    "    for txt, uid in zip(grouped_texts, doc_ids)\n",
    "]\n",
    "retriever.vectorstore.add_documents(text_vectors)\n",
    "\n",
    "# store full Documents (not plain strings!)\n",
    "retriever.docstore.mset(\n",
    "    (uid, Document(page_content=txt, metadata={\"type\": \"text\"}))\n",
    "    for uid, txt in zip(doc_ids, grouped_texts)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "627fd396",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "table_strings = [tbl.text for tbl in tables]                  # convert to str once\n",
    "\n",
    "table_docs = [\n",
    "    Document(page_content=tbl_txt, metadata={id_key: uid, \"type\": \"table\"})\n",
    "    for tbl_txt, uid in zip(table_strings, table_ids)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(table_docs)\n",
    "retriever.docstore.mset(list(zip(table_ids, table_strings))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d571889",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.docstore.mset(\n",
    "    (uid, Document(page_content=txt, metadata={\"type\": \"text\"}))\n",
    "    for uid, txt in zip(doc_ids, grouped_texts)\n",
    ")\n",
    "\n",
    "# --- store tables -----------------------------------------------------------\n",
    "retriever.docstore.mset(\n",
    "    (uid, Document(page_content=tbl_txt, metadata={\"type\": \"table\"}))\n",
    "    for uid, tbl_txt in zip(table_ids, table_strings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4da61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading values\n",
    "\n",
    "# doc_ids = [str(uuid.uuid4()) for _ in grouped_texts]\n",
    "# original_text_docs = [\n",
    "#     Document(page_content=text, metadata={id_key: doc_ids[i]}) for i, text in enumerate(grouped_texts)\n",
    "# ]\n",
    "# retriever.vectorstore.add_documents(original_text_docs)\n",
    "# retriever.docstore.mset(list(zip(doc_ids, grouped_texts)))\n",
    "\n",
    "# # Add tables\n",
    "# tables_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "# original_table_docs = [\n",
    "#     Document(page_content=table, metadata={id_key: tables_ids[i]}) for i, table in enumerate(tables)\n",
    "# ]\n",
    "# retriever.vectorstore.add_documents(original_table_docs)\n",
    "# retriever.docstore.mset(list(zip(tables_ids, tables)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8036424",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93fe4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from base64 import b64decode\n",
    "from prompts import system_finance_prompt\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# def parse_docs(docs):\n",
    "#     # Split base64 images and texts\n",
    "#     b64 = []\n",
    "#     text = []\n",
    "#     for doc in docs:\n",
    "#         try:\n",
    "#             b64decode(doc)\n",
    "#             b64.append(doc)\n",
    "#         except Exception as e:\n",
    "#             text.append(doc)\n",
    "#     return{'images':b64, 'texts':text}\n",
    "\n",
    "def parse_docs(docs):\n",
    "    images, texts = [], []\n",
    "    for doc in docs:                       # doc: langchain_core.documents.Document\n",
    "        payload = doc.page_content         # <-- the actual string you stored\n",
    "        try:\n",
    "            b64decode(payload, validate=True)\n",
    "            images.append(payload)         # looks like a base-64 image\n",
    "        except Exception:\n",
    "            texts.append(payload)          # plain text / table string\n",
    "    return {\"images\": images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def build_prompt_two(kwargs) -> ChatPromptTemplate:\n",
    "    \"\"\"\n",
    "    Construct a ChatPromptTemplate that always begins with the system prompt,\n",
    "    then includes context (text + images) and the user question.\n",
    "    \"\"\"\n",
    "\n",
    "    context = kwargs['context']\n",
    "    question = kwargs['question']\n",
    "\n",
    "    # Concatenate all text fragments\n",
    "    # context_text = \"\".join([t.text for t in context.get('texts', [])])\n",
    "    context_text = \"\\n\".join(context.get(\"texts\", []))\n",
    "    # Build the messages list: SystemMessage -> HumanMessage\n",
    "    messages = [\n",
    "        SystemMessage(content=system_finance_prompt),\n",
    "        HumanMessage(content=f\"Context: {context_text}\\nQuestion: {question}\")\n",
    "    ]\n",
    "\n",
    "    # Include images if present\n",
    "    for b64 in context.get('images', []):\n",
    "        messages.append(\n",
    "            HumanMessage(content={\n",
    "                'type': 'image_url',\n",
    "                'image_url': {'url': f'data:image/jpeg;base64,{b64}'},\n",
    "            })\n",
    "        )\n",
    "\n",
    "    # Create and return a prompt template from these messages\n",
    "    return ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': retriever | RunnableLambda(parse_docs),\n",
    "        'question': RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt_two)\n",
    "    | ChatOpenAI(model='o3')\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    'context': retriever | RunnableLambda(parse_docs),\n",
    "    'question': RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt_two)\n",
    "        | ChatOpenAI(model='o3')\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2f2642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings in collection: 999\n"
     ]
    }
   ],
   "source": [
    "print(\"Total embeddings in collection:\", len(vectorstore._collection.get()[\"ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff8bb966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01645603, -0.0205357 ,  0.01218406, -0.01418955, -0.02765108])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._collection.get(include=[\"embeddings\"])[\"embeddings\"][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4287ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs retrieved\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"Tour Partner Group Limited\")\n",
    "print(len(docs), \"docs retrieved\")\n",
    "for d in docs[:3]:\n",
    "    print(d.page_content[:120], \"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "942b03e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': {'images': [], 'texts': []}, 'question': ' Make the company profile of Tour Partner Group Limited. ', 'response': 'I’m very happy to put together the full restructuring-style one-pager for Tour Partner Group Ltd., however the profile needs the exact figures and wording that appear in the company’s most recent statutory Annual Report (e.g. FY22 or FY23) for every table and most commentary sections.  \\n\\nThat U.K. filing is normally available on Companies House as a PDF, but I don’t have direct access to that document within this chat.  Could you please upload or copy-paste the relevant pages (income statement, cash-flow statement, balance sheet and segmental revenue note), or provide the full FY22 / FY23 Annual Report?  \\n\\nOnce I have those source pages, I can produce the fully-sourced profile with all the required accuracy.'}\n"
     ]
    }
   ],
   "source": [
    "# my_prompt = 'Make the company profile of Tour Partner Group Limited. Even if it is not available: revenue split by geography or segment, full cash-flow statement or EBITDA reconciliation, or other problems.'\n",
    "my_prompt = \"\"\" Make the company profile of Tour Partner Group Limited. \"\"\"\n",
    "profile = chain_with_sources.invoke(my_prompt)\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6355c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m very happy to prepare the full restructuring one-pager, but I need a copy (PDF or image) of Tour Partner Group Limited’s most recent annual report (or at least its primary financial statements and notes covering segment/geographic revenue, debt facilities, cash-flow statement and equity information).  \n",
      "\n",
      "Without that source document I cannot meet the required accuracy standard for:  \n",
      "• exact employee count and operational KPIs  \n",
      "• revenue breakdown that exactly ties to reported totals  \n",
      "• three-year financial highlights table (revenue, margins, cash-flow items, leverage etc.)  \n",
      "• detail on debt facilities, maturity schedule, covenants and liquidity  \n",
      "\n",
      "If you can upload or link the FY23 (or FY22/FY24) statutory accounts filed at Companies House—or any management presentation or interim report that contains the same data—I can immediately build the complete profile with full source citations.\n"
     ]
    }
   ],
   "source": [
    "print(profile['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80d269",
   "metadata": {},
   "source": [
    "# PDF and PPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59191f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from PyPDF2 import PdfReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18b6ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_pdf_unicode(text: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Creates a PDF file from plain text with full Unicode support\n",
    "    using ReportLab's Platypus framework.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: the string content to write into the PDF.\n",
    "    - output_path: full file path where the PDF will be saved.\n",
    "    \"\"\"\n",
    "    # 1. Prepare the document\n",
    "    doc = SimpleDocTemplate(output_path, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    body_style = styles['BodyText']\n",
    "    \n",
    "    # 2. Build a \"story\" of flowable objects\n",
    "    story = []\n",
    "    for line in text.split('\\n'):\n",
    "        # Paragraph handles Unicode (e.g. “–”, “é”, emojis, etc.) natively\n",
    "        story.append(Paragraph(line or ' ', body_style))\n",
    "        # Small spacer between lines\n",
    "        story.append(Spacer(1, 4))\n",
    "    \n",
    "    # 3. Generate the PDF\n",
    "    doc.build(story)\n",
    "\n",
    "profile_text = profile[\"response\"]  # fetched from your chain output\n",
    "text_to_pdf_unicode(profile_text, \"/Users/felipesilverio/Documents/GitHub/LangChainTest/output/company_profile_unicode2.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4491a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx.util import Pt\n",
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "def replace_placeholder_text(file_path: str,placeholder: str,replacement: str,output_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Replace occurrences of `placeholder` in text elements with `replacement`.\n",
    "\n",
    "    Modifies the presentation and saves to `output_path` or overwrites original.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prs = Presentation(file_path)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Unable to open file {file_path}: {e}\")\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame:\n",
    "                continue\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                for run in paragraph.runs:\n",
    "                    if placeholder in run.text:\n",
    "                        run.text = run.text.replace(placeholder, replacement)\n",
    "                        run.font.size = Pt(5)\n",
    "\n",
    "    save_path = output_path or file_path\n",
    "    try:\n",
    "        prs.save(save_path)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Unable to save updated file to {save_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7de6ac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Company Profile – Tour Partner Group Limited\\nSources utilised:\\n(1) Companies House – “Tour Partner Group Limited, Annual Report & Financial Statements for the\\nyear-ended 31-Dec-22”, filed 29-Sep-23 (pdf)\\n(2) Tour Partner Group website – “About Us” page, accessed 07-Jun-24\\n--------------------------------------------------\\n1. Company Snapshot\\nPrimary Industry: Travel & Tourism\\nIncorporation Year: 2016\\nHeadquarters: London, United Kingdom\\nEmployees: 230 (FY22, Companies House filing, note 6)\\nOperational KPIs (FY22)\\n\\x7f Passengers handled: 350,000\\n\\x7f Bed-nights booked: 710,000\\n\\x7f Source markets served: 70+\\n(Sources: (1) Directors’ report & strategic review; (2) corporate website)\\n--------------------------------------------------\\n2. Business Overview (bullets only)\\n\\x7f Tour Partner Group Limited is an intermediate holding company that consolidates a portfolio of B2B\\ndestination management and group travel brands serving the UK & Ireland, the Nordics and\\nContinental Europe.\\n\\x7f The company, through subsidiaries such as Hotels & More, Irish Welcome Tours, Authentic Vacations\\nand Trans Nordic Tours, designs tailor-made tours for coach operators, tour operators and wholesalers\\nworldwide.\\n\\x7f It maintains a network of nine operational offices across six European countries, giving it strong\\non-the-ground contracting capabilities and local destination expertise.\\n\\x7f Tour Partner Group markets more than 5,000 hotel partners and leverages preferred rates to package\\nmulti-day itineraries covering accommodation, attractions, transport and guiding services.\\n\\x7f The company’s scale allows it to handle c. 350k passengers and to contract in excess of 700k\\nbed-nights annually, positioning it among the larger DMC platforms in Europe.\\n\\n\\x7f It reports that over 80 % of business is repeat custom, benefitting from long-term relationships with\\nNorth-American, Asian and European wholesale partners.\\n\\x7f The group continues to invest in proprietary booking technology aimed at shortening quotation times\\nand improving margin through dynamic pricing.\\n\\x7f It states that the 2023/24 strategic focus is margin recovery following Covid-19, alongside bolt-on\\nacquisitions that extend destination coverage or deepen source-market penetration.\\n(Source: (1) Strategic Report; (2) About-Us webpage)\\n--------------------------------------------------\\n3. Revenue Split – NOT DETECTED\\nThe FY22 annual report does not disclose segmental or geographic revenue information.\\n--------------------------------------------------\\n4. Key Stakeholders Table – NOT DETECTED\\nThe filed FY22 holding-company accounts are prepared under FRS 102 Section 1A and do not include\\na list of ultimate shareholders, detailed director biographies or any lender disclosures.\\n--------------------------------------------------\\n5. Financial Highlights – NOT DETECTED (Cash-flow statement absent)\\nThe FY22 statutory accounts are single-column holding-company statements prepared under the\\n“company-only” exemption and contain no consolidated profit & loss or cash-flow information.\\n--------------------------------------------------\\n6. Capital Structure – NOT DETECTED (No debt note)\\nThe FY22 accounts state that the company has no external bank borrowings and therefore do not\\nprovide a debt maturity schedule or covenant disclosures.\\n--------------------------------------------------\\nMaterial information gaps have been highlighted above in accordance with the instruction to flag absent\\n(i) segment/geographic revenue split, (ii) cash-flow statement, (iii) debt note and (iv)\\nshareholder/director disclosures.\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader('/Users/felipesilverio/Documents/GitHub/LangChainTest/output/company_profile_unicode.pdf')\n",
    "reader\n",
    "\n",
    "all_text = []\n",
    "for page_num, page in enumerate(reader.pages, start=1):\n",
    "    try:\n",
    "        text = page.extract_text() or ''\n",
    "    except Exception:\n",
    "        text = ''\n",
    "    all_text.append(text)\n",
    "\n",
    "pdf_retrieved = \"\\n\".join(all_text)\n",
    "pdf_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4251fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_path = '/Users/felipesilverio/Documents/GitHub/LangChainTest/try2.pptx'\n",
    "my_prompt = f\"\"\"\n",
    "The following text contains a series of text blocks that are separated by multiples - like:\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Bring me back only the code block relative to 1. Company Snapshot, and give me all the information in it in a single line but separated by |\n",
    "\n",
    "The text is:\n",
    "\n",
    "{pdf_retrieved}\n",
    "\"\"\"\n",
    "response = chain_with_sources.invoke(my_prompt)\n",
    "replace_placeholder_text('/Users/felipesilverio/Documents/GitHub/LangChainTest/backupppt.pptx', 'Company Snapshot', response['response'], ppt_path)\n",
    "\n",
    "my_prompt = f\"\"\"\n",
    "The following text contains a series of text blocks that are separated by multiples - like:\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Bring me back only the code block relative to Business Overview, and give me back exactly as is in the text.\n",
    "\n",
    "The text is:\n",
    "\n",
    "{pdf_retrieved}\n",
    "\"\"\"\n",
    "response = chain_with_sources.invoke(my_prompt)\n",
    "replace_placeholder_text('/Users/felipesilverio/Documents/GitHub/LangChainTest/backupppt.pptx', 'Business Overview Text', response['response'], ppt_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
